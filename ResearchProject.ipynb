{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BucketofJava/Generative-Emotion-Network/blob/main/ResearchProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LsI1CSx-a1v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.width', 17000)\n",
        "df=pd.read_csv(\"emotion-labels-train.csv\");\n",
        "df=df.sample(n=8)\n",
        "\n",
        "df.head(n=8)\n"
      ],
      "metadata": {
        "id": "oowsJNliLzdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pvg2Vye0MvG"
      },
      "source": [
        "# Import and format text and emotion data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aieGiMB8SGyf"
      },
      "outputs": [],
      "source": [
        "whitelistchars=['\\n', ' ', '!', '\"', \"'\", ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '\\\\', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', \n",
        "                'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~', '\\xa0', '£', '´', '»', 'é', 'ñ', 'ó','–', '—', '―', '‘', '’', '“', '”', '•', '…', '‼', '™']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo1Bg_DKsBgp"
      },
      "outputs": [],
      "source": [
        "texts=[]\n",
        "emotions=[]\n",
        "emotionlabels=[]\n",
        "emotionlist=[\"joy\", \"sadness\", \"anger\", \"fear\"]\n",
        "f=open(\"/content/emotion-labels-train.csv\", \"r\")\n",
        "current_line=f.readline()\n",
        "def processtweet(tweet):\n",
        "  twot=tweet.split(\" \")\n",
        "  toremove=[]\n",
        "  hashtags=[]\n",
        "  for i in range(len(twot)):\n",
        "    blacklistedchars=\"#$%^&*()_+{}[]\"\n",
        "    if(\"@\" in twot[i]):\n",
        "      toremove.append(i)\n",
        "    if(\"#\" in twot[i]):\n",
        "      hashtags.append(i)\n",
        "    else:\n",
        "      hashtags=[]\n",
        "    for j in twot[i]:\n",
        "      if(not j.lower() in whitelistchars):\n",
        "        blacklistedchars=blacklistedchars+j\n",
        "    for j in blacklistedchars:\n",
        "      twot[i]=twot[i].replace(j, '')\n",
        "  toremove=toremove+hashtags\n",
        "  for i in range(len(toremove)):\n",
        "    twot.pop(toremove[i]-i)\n",
        "  return \" \".join(twot)\n",
        "while(\",\" in current_line):\n",
        "  row=current_line.split(\",\")\n",
        "  labelname=row[len(row)-1].lower().replace(\" \", \"\").replace(\"\\n\", \"\")\n",
        "  if(not labelname in emotionlist):\n",
        "    current_line=f.readline()\n",
        "    continue\n",
        "  texts.append(processtweet(\",\".join(row[:len(row)-1])))\n",
        "  emotionlabels.append(row[len(row)-1].lower().replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
        "  emotions.append(emotionlist.index(row[len(row)-1].lower().replace(\" \", \"\").replace(\"\\n\", \"\")))\n",
        "  current_line=f.readline()\n",
        "f.close()\n",
        "f=open(\"/content/emotion-labels-test.csv\")\n",
        "current_line=f.readline()\n",
        "while(\",\" in current_line):\n",
        "  row=current_line.split(\",\")\n",
        "  labelname=row[len(row)-1].lower().replace(\" \", \"\").replace(\"\\n\", \"\")\n",
        "  if(not labelname in emotionlist):\n",
        "    current_line=f.readline()\n",
        "    continue\n",
        "  texts.append(processtweet(\",\".join(row[:len(row)-1])))\n",
        "  emotionlabels.append(row[len(row)-1].lower().replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
        "  emotions.append(emotionlist.index(row[len(row)-1].lower().replace(\" \", \"\").replace(\"\\n\", \"\")))\n",
        "  current_line=f.readline()\n",
        "f.close()                                                                                                                                                                                                                                                                                                                                                                    \n",
        "#index=int(len(texts)*.8)\n",
        "print(len(emotions))\n",
        "from keras.utils.np_utils import to_categorical\n",
        "emotions = to_categorical(emotions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjUWNHS5lzyp"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "texts_new=texts.copy()\n",
        "for i in range(len(texts)):\n",
        "  endstring=\"ENDTOKEN\"\n",
        "  if(not texts[i][len(texts[i])-1]==\" \"):\n",
        "    endstring=\" ENDTOKEN\"\n",
        "  texts_new[i]=\"STARTOKEN \"+texts[i]+endstring;\n",
        "gen_tokenizer=Tokenizer(num_words=10000)\n",
        "gen_tokenizer.fit_on_texts(texts_new);\n",
        "token_sequences=gen_tokenizer.texts_to_sequences(texts_new);\n",
        "token_list_states=[]\n",
        "token_actions=[]\n",
        "gan_list_states=[]\n",
        "gan_classification_onehots=[]\n",
        "gan_actions=[]\n",
        "classification_onehots=[]\n",
        "for token_sequence_index in range(len(token_sequences)):\n",
        "  token_sequence=token_sequences[token_sequence_index]\n",
        "\n",
        "  for i in range(1, len(token_sequence)):\n",
        "    token_list_states.append(token_sequence[:i])\n",
        "    classification_onehots.append(emotions[token_sequence_index])\n",
        "    token_actions.append([token_sequence[i]])\n",
        "    if(i+1>=len(token_sequence)):\n",
        "      gan_list_states.append(token_sequence[:i])\n",
        "     # print(token_sequence)\n",
        "      gan_classification_onehots.append(emotions[token_sequence_index])\n",
        "      gan_actions.append([token_sequence[i]])\n",
        "token_list_states=np.array(pad_sequences(token_list_states, maxlen=64))\n",
        "gan_list_states=np.array(pad_sequences(gan_list_states, maxlen=64))\n",
        "classification_onehots=np.array(classification_onehots)\n",
        "gan_classification_onehots=np.array(gan_classification_onehots)\n",
        "print(gen_tokenizer.word_index)\n",
        "token_actions=to_categorical(np.stack(token_actions, axis=0), num_classes=len(gen_tokenizer.word_index))\n",
        "gan_actions=to_categorical(np.stack(gan_actions, axis=0), num_classes=len(gen_tokenizer.word_index))\n",
        "print(token_actions)\n",
        "print(classification_onehots)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTQDKmSaub0P"
      },
      "source": [
        "#Classifier Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=np.array(pad_sequences(token_sequences, maxlen=200))\n",
        "y_train=np.array(emotions)\n",
        "shuffled_indices=np.arange(6755)\n",
        "np.random.shuffle(shuffled_indices)\n",
        "x_train=x_train[shuffled_indices]\n",
        "y_train=y_train[shuffled_indices]"
      ],
      "metadata": {
        "id": "CF0wPmeJiKl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gBWBJtWAmmj"
      },
      "outputs": [],
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense, Maximum, LSTM, GRU, Conv1D, MaxPooling1D, Bidirectional\n",
        "\n",
        "# print(len(emotionlabels))\n",
        "# print(len(texts))\n",
        "# print(x_train)\n",
        "# print(y_train)\n",
        "# print(x_train.shape)\n",
        "# print(y_train.shape)\n",
        "classifier = Sequential()\n",
        "classifier.add(Embedding(12844, 100, name=\"Embedding\"))\n",
        "classifier.add(Conv1D(32, 2, name=\"Conv1D\")) \n",
        "classifier.add(Bidirectional(LSTM(100, name=\"LSTM\"), name=\"Bidirectional\"))\n",
        "classifier.add(Dense(100, activation='relu', name=\"Dense-ReLU\"))\n",
        "classifier.add(Dense(4, activation='softmax', name=\"Dense-Softmax\"))\n",
        "classifier.summary()\n",
        "classifier.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "metrics=['acc']) \n",
        "stop_index=int(len(x_train)*.95)\n",
        "# print(stop_index/len(x_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(classifier, show_layer_activations=True, show_layer_names=False, show_shapes=True, rankdir=\"LR\")"
      ],
      "metadata": {
        "id": "dC7vrICuj_ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "absldyDkuMzZ"
      },
      "outputs": [],
      "source": [
        "classifier = Sequential()\n",
        "classifier.add(Embedding(12844, 100))\n",
        "classifier.add(Conv1D(32, 4, padding=\"causal\")) \n",
        "classifier.add(Bidirectional(LSTM(100)))\n",
        "classifier.add(Dense(100, activation='relu'))\n",
        "classifier.add(Dense(4, activation='softmax'))\n",
        "classifier.summary()\n",
        "classifier.compile(optimizer='rmsprop', \n",
        "loss='categorical_crossentropy',\n",
        "metrics=['acc']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H-VWqNTRBq2"
      },
      "outputs": [],
      "source": [
        "history = classifier.fit(x_train[:stop_index], y_train[:stop_index], \n",
        "                         epochs=8, batch_size=32, validation_split=0.05)\n",
        "score=classifier.evaluate(x_train[stop_index:],\n",
        "                          y_train[stop_index:])\n",
        "\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVJ10TNHa4ur"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        "#plt.plot(history.history['acc'])\n",
        "plt.plot([0]+history.history['val_acc'])\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.grid(True)\n",
        "plt.xlim(1, 8)\n",
        "plt.ylim(0.5, 0.9)\n",
        "plt.title(\"Validation Accuracy Over Training Time\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShKrOkXX-g-P"
      },
      "outputs": [],
      "source": [
        "from keras.models import save_model\n",
        "classifier.save(\"/content/drive/MyDrive/Saved_Models/classifier_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg2tYfOz_Q34"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "classifier=load_model(\"/content/drive/MyDrive/Saved_Models/classifier_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48uGVTyQtXkq"
      },
      "outputs": [],
      "source": [
        "def classify_text(text):\n",
        "  text=[text]\n",
        "  text=tokenizer.texts_to_sequences(text)\n",
        "  text=pad_sequences(text, maxlen=maxlen)\n",
        "  return emotionlist[classifier.predict(text).argmax()]\n",
        "def text_probs(text):\n",
        "  text=[text]\n",
        "  text=tokenizer.texts_to_sequences(text)\n",
        "  text=pad_sequences(text, maxlen=maxlen)\n",
        "  return model.predict(text)\n",
        "def classify_texts(texts):\n",
        "  texts=tokenizer.texts_to_sequences(texts)\n",
        "  texts=pad_sequences(texts, maxlen=maxlen)\n",
        "  return classifier.predict(texts)\n",
        "#classify_text(\"Put something here\")\n",
        "xx=x_train[stop_index:]\n",
        "yy=y_train[stop_index:]\n",
        "o=model.predict(xx)\n",
        "joy=[0, 0]\n",
        "anger=[0, 0]\n",
        "sadness=[0, 0]\n",
        "fear=[0, 0]\n",
        "emdict={\"joy\":joy, \"anger\":anger, \"sadness\":sadness, \"fear\":fear}\n",
        "for i in range(len(o)):\n",
        "  emdict[emotionlist[yy[i].argmax(axis=0)]][0]=emdict[emotionlist[yy[i].argmax(axis=0)]][0]+1\n",
        "  if(o[i].argmax(axis=0)==yy[i].argmax(axis=0)):\n",
        "    emdict[emotionlist[yy[i].argmax(axis=0)]][1]=emdict[emotionlist[yy[i].argmax(axis=0)]][1]+1\n",
        "print(\"joy: {}%\\nanger: {}%\\nsadness: {}%\\nfear: {}%\".format(joy[1]/joy[0], anger[1]/anger[0], sadness[1]/sadness[0], fear[1]/fear[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jOOe5KKzESW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chBzr0bfPyFC",
        "outputId": "6e96d739-b096-4fb1-f8a4-dfa30cc5c280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anger\n"
          ]
        }
      ],
      "source": [
        "print(classify_text(\"I hate you\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIrXVIwuy6-m",
        "outputId": "4c2ef186-891d-4c77-faa0-6313704a1177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fear\n"
          ]
        }
      ],
      "source": [
        "print(classify_text(\"He was large and menacing\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_V_zn3ly-YZ",
        "outputId": "b6447b69-d3c9-4262-b56a-c823b3f20411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sadness\n"
          ]
        }
      ],
      "source": [
        "print(classify_text(\"Life feels meaningless at the moment\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x0z68uz1C1o",
        "outputId": "f3e343d5-3964-4254-a6a9-8308f93cb94b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "joy\n"
          ]
        }
      ],
      "source": [
        "print(classify_text(\"Today is a beautiful day\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNtRJUPg1pnJ"
      },
      "source": [
        "# Re-tokenize and format data for the generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_Ke5dNR_tkA"
      },
      "outputs": [],
      "source": [
        "texts_new=texts.copy()\n",
        "for i in range(len(texts)):\n",
        "  endstring=\"ENDTOKEN\"\n",
        "  if(not texts[i][len(texts[i])-1]==\" \"):\n",
        "    endstring=\" ENDTOKEN\"\n",
        "  texts_new[i]=\"STARTOKEN \"+texts[i]+endstring;\n",
        "gen_tokenizer=Tokenizer(num_words=10000)\n",
        "gen_tokenizer.fit_on_texts(texts_new);\n",
        "token_sequences=gen_tokenizer.texts_to_sequences(texts_new);\n",
        "token_list_states=[]\n",
        "token_actions=[]\n",
        "gan_list_states=[]\n",
        "gan_classification_onehots=[]\n",
        "gan_actions=[]\n",
        "classification_onehots=[]\n",
        "for token_sequence_index in range(len(token_sequences)):\n",
        "  token_sequence=token_sequences[token_sequence_index]\n",
        "\n",
        "  for i in range(1, len(token_sequence)):\n",
        "    token_list_states.append(token_sequence[:i])\n",
        "    classification_onehots.append(emotions[token_sequence_index])\n",
        "    token_actions.append([token_sequence[i]])\n",
        "    if(i+1>=len(token_sequence)):\n",
        "      gan_list_states.append(token_sequence[:i])\n",
        "     # print(token_sequence)\n",
        "      gan_classification_onehots.append(emotions[token_sequence_index])\n",
        "      gan_actions.append([token_sequence[i]])\n",
        "token_list_states=np.array(pad_sequences(token_list_states, maxlen=64))\n",
        "gan_list_states=np.array(pad_sequences(gan_list_states, maxlen=64))\n",
        "classification_onehots=np.array(classification_onehots)\n",
        "gan_classification_onehots=np.array(gan_classification_onehots)\n",
        "print(gen_tokenizer.word_index)\n",
        "token_actions=to_categorical(np.stack(token_actions, axis=0), num_classes=len(gen_tokenizer.word_index))\n",
        "gan_actions=to_categorical(np.stack(gan_actions, axis=0), num_classes=len(gen_tokenizer.word_index))\n",
        "print(token_actions)\n",
        "print(classification_onehots)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0J_-F-2umOw"
      },
      "source": [
        "#Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBq2nsVbx-T8"
      },
      "outputs": [],
      "source": [
        "from keras.layers import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sAM3r7JjohT"
      },
      "outputs": [],
      "source": [
        "\n",
        "from keras import Model\n",
        "classification_label=Input(shape=(4,), name=\"labels\")\n",
        "classification_result=Dense(64)(classification_label)\n",
        "token_input=Input(shape=(64,), name=\"sequences\")\n",
        "token_result=Embedding(len(gen_tokenizer.word_index), 100)(token_input)\n",
        "token_result=Conv1D(32, 2)(token_result)\n",
        "token_result=Reshape((63, 32))(token_result)\n",
        "token_result=GRU(64)(token_result)\n",
        "concatenated_inputs=Concatenate(axis=1)([classification_result, token_result])\n",
        "concatenated_inputs=Dense(len(gen_tokenizer.word_index), activation=\"softmax\")(concatenated_inputs)\n",
        "generator_model=Model(inputs=[classification_label, token_input], outputs=concatenated_inputs)\n",
        "generator_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "#generator_model.fit(x={\"labels\":classification_onehots, \"sequences\":token_list_states}, y=token_actions, validation_split=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator_model.fit(x={\"labels\":classification_onehots, \"sequences\":token_list_states}, y=token_actions, validation_split=0.05)"
      ],
      "metadata": {
        "id": "OGctzqELjfs1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e827ff-9aad-4ded-c8d6-6697eb623760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3110/3110 [==============================] - 227s 72ms/step - loss: 6.8605 - acc: 0.0790 - val_loss: 6.3212 - val_acc: 0.0911\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5c9a85cdd0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(generator_model, show_layer_activations=True, show_layer_names=False, show_shapes=True, rankdir=\"LR\")"
      ],
      "metadata": {
        "id": "AZnxLBnCjnaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_model.load_weights(\"/content/drive/MyDrive/Saved_Models/Feb 28/generator_weights.h5\")"
      ],
      "metadata": {
        "id": "DqxXgPqh7fAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fG599C1B7pHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaVNkKGr_a16"
      },
      "outputs": [],
      "source": [
        "from keras.models import save_model\n",
        "generator_model.save(\"/content/drive/MyDrive/Saved_Models/generator_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-nDZjbt_kWi"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "generator_model=load_model(\"/content/drive/MyDrive/Saved_Models/generator_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb80vxie9C-N"
      },
      "outputs": [],
      "source": [
        "print(gen_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLRf35tnFQPS"
      },
      "outputs": [],
      "source": [
        "s=\"Emil is \"\n",
        "for i in range(20):\n",
        "   s+=\" \"+list(gen_tokenizer.word_index.keys())[roll_index(generator_model.predict(x={\"labels\":np.array([[0, 0.6, 0.4, 0]]), \n",
        "                                  \"sequences\":pad_sequences(gen_tokenizer.texts_to_sequences([s]), \n",
        "                                                            maxlen=64)})[0])]\n",
        "print(s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuOXtQ5_L8RV"
      },
      "source": [
        "#Define Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQtqQdcat4lt"
      },
      "source": [
        "##Generator Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmJABGbmL7WX"
      },
      "outputs": [],
      "source": [
        "class Generator():\n",
        "  def __init__(self, generator_model, generator_loss_function, generator_optimizer):\n",
        "    self.generator_model=generator_model;\n",
        "    self.generator_loss_function=generator_loss_function;\n",
        "    self.generator_optimizer=generator_optimizer;\n",
        "  def pre_train(self, x_data, classification_target, y_data, g_epochs):\n",
        "    self.generator_model.fit([classification_target, x_data], y_data, epochs=g_epochs)\n",
        "  def computePolicyGradient(self, states, classification_targets, aprobs, rewards, learning_rate):\n",
        "    with tf.GradientTape() as tape:\n",
        "      newaprobs=self.generator_model([classification_targets, states])\n",
        "      generator_loss=self.generator_loss_function(aprobs*rewards, newaprobs)\n",
        "      generator_loss+=self.generator_model.losses\n",
        "    generator_gradient=tape.gradient(generator_loss, self.generator_model.trainable_weights)\n",
        "    self.generator_optimizer.apply_gradients(zip(generator_gradient, self.generator_model.trainable_weights))\n",
        "  def getScore(self, classification_targets, sequence_data, y_data):\n",
        "    return self.generator_model.evaluate({\"labels\":classification_targets, \"sequences\":sequence_data}, y_data)\n",
        "  def generateBatch(self, amount, seed_set=None, g_targets=None):\n",
        "    seeds=seed_set;\n",
        "    if(seed_set==None):\n",
        "      basic_seed=np.zeros(64)\n",
        "      basic_seed[63]=1;\n",
        "      basic_seed=np.array([basic_seed])\n",
        "      seeds=np.repeat(basic_seed, amount, axis=0)\n",
        "    generator_targets=g_targets\n",
        "    if(g_targets==None):\n",
        "      generator_targets=[]\n",
        "      for i in range(amount):\n",
        "        generator_targets.append(getRandomProbabilityVector(4));\n",
        "      generator_targets=np.array(generator_targets)\n",
        "    #print(seeds.shape)\n",
        "    #print(generator_targets.shape)\n",
        "    generator_results=[]\n",
        "    for i in range(len(seeds)):\n",
        "      print(i)\n",
        "      seed=seeds[i] \n",
        "      c=0;\n",
        "      current_sequence=seed[seed!=0]\n",
        "      \n",
        "      while c<20:\n",
        "        generated_token=roll_index(self.generator_model.predict_on_batch({\"labels\": np.array([generator_targets[i]]), \"sequences\": pad_sequences([current_sequence], maxlen=64)})[0])\n",
        "        #print(generated_token)\n",
        "        if(c>=19):\n",
        "          generated_token=2\n",
        "        current_sequence=np.append(current_sequence, generated_token)\n",
        "        if(generated_token==2):\n",
        "          break;\n",
        "        c+=1;\n",
        "      generator_results.append(current_sequence)\n",
        "    #print(generator_results)\n",
        "    generator_results=pad_sequences(np.array(generator_results), maxlen=64)\n",
        "    return generator_results, generator_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9zv5Lmlt-Wi"
      },
      "source": [
        "##Discriminator Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqOaLaUdQCts"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "class Discriminator():\n",
        "  def __init__(self, discriminator_model, discriminator_loss_function, discriminator_optimizer):\n",
        "    self.discriminator_model=discriminator_model;\n",
        "\n",
        "    self.discrimnator_loss_function=discriminator_loss_function;\n",
        "    self.discriminator_optimizer=discriminator_optimizer\n",
        "  def train(self, real_x, real_y, generator, d_split, d_epochs=1):\n",
        "    y_tokens=[]\n",
        "    for i in (real_y):\n",
        "       y_tokens.append(roll_index(i))\n",
        "    y_tokens=np.array([y_tokens]).T\n",
        "    x_data_real=np.concatenate((real_x[1], y_tokens), axis=1)\n",
        "    x_data_real=np.delete(x_data_real, 0, axis=1)\n",
        "    x_data_fake=generator.generateBatch(real_x[1].shape[0])[0]\n",
        "    for i in range(d_epochs):\n",
        "      real_variance=[]\n",
        "      fake_variance=[]\n",
        "      for i in range(int(x_data_real.shape[0])):\n",
        "        real_variance.append(random.randint(0, 20)/100)\n",
        "        fake_variance.append(random.randint(0, 20)/100)\n",
        "      np.random.shuffle(real_variance)\n",
        "      np.random.shuffle(fake_variance) \n",
        "      y_data_real, y_data_fake=np.ones(x_data_real.shape[0])-real_variance, np.zeros(x_data_fake.shape[0])+fake_variance\n",
        "      current_index=0\n",
        "      length_fraction_real=int(x_data_real.shape[0]/d_split)\n",
        "      length_fraction_fake=int(x_data_fake.shape[0]/d_split)\n",
        "      index_real_previous=0\n",
        "      index_fake_previous=0\n",
        "      for i in range(d_split+1):\n",
        "        index_real=(i*length_fraction_real)+random.randint(0, length_fraction_real)\n",
        "        index_fake=(i*length_fraction_real)+random.randint(0, length_fraction_fake)\n",
        "        if(i>=d_split):      \n",
        "          self.discriminator_model.train_on_batch(x_data_real[index_real_previous:], y_data_real[index_real_previous:])\n",
        "          self.discriminator_model.train_on_batch(x_data_fake[index_fake_previous:], y_data_fake[index_fake_previous:])\n",
        "        self.discriminator_model.train_on_batch(x_data_real[index_real_previous:index_real], y_data_real[index_real_previous:index_real])\n",
        "        self.discriminator_model.train_on_batch(x_data_fake[index_fake_previous:index_fake], y_data_fake[index_fake_previous:index_fake])\n",
        "  def evaluate_discriminator(self, real_states, real_actions, generator):\n",
        "    action_tokens=[]\n",
        "    for i in (real_actions):\n",
        "       action_tokens.append(roll_index(i))\n",
        "    action_tokens=np.array([action_tokens]).T\n",
        "    x_data_real=np.concatenate((real_states, action_tokens), axis=1)\n",
        "    x_data_real=np.delete(x_data_real, 0, axis=1)\n",
        "    x_data_fake=generator.generateBatch(x_data_real.shape[0])[0]\n",
        "    real_variance=pad_sequences([np.ones(int(x_data_real.shape[0]/5))/100], maxlen=x_data_fake.shape[0])[0]\n",
        "    fake_variance=pad_sequences([np.ones(int(x_data_fake.shape[0]/5))/100], maxlen=x_data_real.shape[0])[0]\n",
        "    np.random.shuffle(real_variance)\n",
        "    np.random.shuffle(fake_variance) \n",
        "    y_data_real, y_data_fake=np.ones(x_data_real.shape[0])-real_variance, np.zeros(x_data_fake.shape[0])+fake_variance\n",
        "    y_data=np.concatenate((y_data_real, y_data_fake), axis=0)\n",
        "    x_data=np.concatenate((x_data_real, x_data_fake), axis=0)\n",
        "    return self.discriminator_model.evaluate(x_data, y_data);\n",
        "  def compute_reward(self, sequence):\n",
        "    print(self.discriminator_model.predict(np.array([sequence]))[0])\n",
        "    return self.discriminator_model.predict(np.array([sequence]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VBpTZ3zzIlO"
      },
      "source": [
        "#Define Discriminator Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EXrrnSzdEu5"
      },
      "outputs": [],
      "source": [
        "#Createas discriminator model\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import adam_v2\n",
        "discriminator=Sequential()\n",
        "discriminator.add(Input(64,))\n",
        "discriminator.add(layers.Embedding(len(gen_tokenizer.word_index), 100))\n",
        "discriminator.add(layers.Conv1D(32, 2))\n",
        "discriminator.add(layers.LSTM(32))\n",
        "discriminator.add(layers.Dense(32, activation='relu'))\n",
        "discriminator.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "discriminator.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SImFGB1ww2h7"
      },
      "outputs": [],
      "source": [
        "#Createas discriminator model\n",
        "from keras import layers\n",
        "from keras.layers import Input\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import adam_v2\n",
        "discriminator=Sequential()\n",
        "discriminator.add(Input(64,))\n",
        "discriminator.add(layers.Embedding(12844, 100))\n",
        "discriminator.add(layers.LSTM(32, return_sequences=True))\n",
        "discriminator.add(layers.Conv1D(32, 2))\n",
        "discriminator.add(layers.Conv1D(32, 2))\n",
        "discriminator.add(layers.Conv1D(32, 2))\n",
        "discriminator.add(layers.Conv1D(32, 2))\n",
        "discriminator.add(layers.Conv1D(32, 2))\n",
        "discriminator.add(layers.Conv1D(32, 2))\n",
        "discriminator.add(layers.Conv1D(32, 2))\n",
        "discriminator.add(layers.Conv1D(32, 2))\n",
        "discriminator.add(layers.Conv1D(32, 2))\n",
        "discriminator.add(layers.Conv1D(32, 2))\n",
        "discriminator.add(layers.Conv1D(32, 2))\n",
        "discriminator.add(layers.Conv1D(32, 2))\n",
        "discriminator.add(layers.Bidirectional(layers.LSTM(32)))\n",
        "discriminator.add(layers.Dense(32, activation='relu'))\n",
        "discriminator.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "discriminator.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(discriminator, show_layer_activations=True, show_layer_names=False, show_shapes=True, rankdir=\"LR\")"
      ],
      "metadata": {
        "id": "R0GLPvAlkBPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(discriminator)"
      ],
      "metadata": {
        "id": "OnoeL-hynP3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnZaREf70XYP"
      },
      "outputs": [],
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(classifier, show_shapes=True, show_dtype=False, show_layer_activations=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHrO1E2uteM6"
      },
      "source": [
        "\n",
        "#Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cugEqTOptd8i"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def getRandomProbabilityVector(length):\n",
        "  probabilityVector=[]\n",
        "  r=0\n",
        "  for i in range(length):\n",
        "    r_increase=random.random();\n",
        "    r+=r_increase\n",
        "    probabilityVector.append(r_increase)\n",
        "  probabilityVector=np.array(probabilityVector)/r\n",
        "  return probabilityVector\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDtb7ESDMYDf"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def classifier_loss(classifier_output, classifier_target, weight=1):\n",
        "  current_output_total=0;\n",
        "  current_target_total=0;\n",
        "  total_loss=0;\n",
        "  weight_total=0\n",
        "  for i in range(len(classifier_output)):\n",
        "    if(current_output_total==1): current_output_total-=0.01;\n",
        "    if current_target_total==1: current_target_total-=0.01;\n",
        "    #print(current_output_total)\n",
        "    #print(classifier_output[i])\n",
        "    #print(abs((classifier_output[i]/(1-current_output_total))-(classifier_target[i]/(1-current_target_total))))\n",
        "    total_loss+=(weight**i)*abs((classifier_output[i]/(1-current_output_total))-(classifier_target[i]/(1-current_target_total)))\n",
        "    weight_total+=(weight**i)\n",
        "    current_output_total+=classifier_output[i];\n",
        "    current_target_total+=classifier_target[i];\n",
        "  \n",
        "  print(\"Total Loss: {}\".format(total_loss))\n",
        "\n",
        "  total_loss=total_loss*(len(classifier_output)/weight_total)\n",
        "  \n",
        "  return total_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDNtPQ_EDZ7F"
      },
      "outputs": [],
      "source": [
        "for i in range(100):\n",
        "  x=getRandomProbabilityVector(4)\n",
        "  y=getRandomProbabilityVector(4)\n",
        "  z=classifier_loss(x,y)\n",
        "  if(z>1 or z<0):\n",
        "    print(x)\n",
        "    print(y)\n",
        "    print(classifier_loss(x, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzrZ_FwWMmAc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def roll_index(prob_list):\n",
        "  r=random.random()\n",
        "  sum=0\n",
        "  for i in range(len(prob_list)):\n",
        "    sum+=prob_list[i]\n",
        "    if(sum>=r):\n",
        "      return i;\n",
        "  return 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX1BtTmQLr7U"
      },
      "source": [
        "#GAN class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zSaTquOtGR8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.eager.context import get_executor\n",
        "import math\n",
        "class GAN():\n",
        "  def __init__(self, generator, discriminator, classifier, classifier_loss):\n",
        "    self.generator=generator\n",
        "    self.discriminator=discriminator\n",
        "    self.classifier=classifier\n",
        "    self.classifier_loss=classifier_loss\n",
        "  def pre_train_gen(self, g_x, g_y, g_epochs):\n",
        "    self.generator.pre_train(g_x, g_y, g_epochs);\n",
        "  def train_disc(self, g_x, g_targets, g_y, d_split, d_epochs=5):\n",
        "    self.discriminator.train([g_targets, g_x], g_y, self.generator, d_split, d_epochs);\n",
        "  def train_gen(self, g_x, g_classifier_targets, g_y, g_epochs, state_num, mc_num, end_token, max_seq_len=100, reward_constant=1):\n",
        "    print(\"Beginning training\")\n",
        "\n",
        "    aprobs=self.generator.generator_model.predict_on_batch({\"labels\":g_classifier_targets, \"sequences\":g_x});\n",
        "  \n",
        "    final_rewards=[]\n",
        "    for i in range(min(len(g_x), state_num)):\n",
        "      print(\"Action probabilities generated\")\n",
        "      g_x_mc=g_x[np.random.shuffle(np.arange(len(g_x)))][0]\n",
        "      rewards=np.ones(len(aprobs[0]))\n",
        "      states=[]\n",
        "      actions=[]\n",
        "      generator_targets=[]\n",
        "      rewards_=[] \n",
        "      print(\"Beginning monte carlo search\")     \n",
        "      print(\"Starting state\")\n",
        "      print(g_x_mc)\n",
        "      current_start_sequence=g_x_mc[i]\n",
        "      current_start_sequence=current_start_sequence[current_start_sequence!=0]\n",
        "      for seed in range(1, len(current_start_sequence)):\n",
        "        print(\"Starting seed\")\n",
        "        current_seed_sequence=current_start_sequence[:(seed)]\n",
        "        states.append(current_seed_sequence)\n",
        "        print(len(aprobs))\n",
        "        aprob=np.zeros(len(aprobs[0]));\n",
        "        aprobi=current_start_sequence[seed]\n",
        "        aprob[aprobi]=1;\n",
        "        actions.append(aprob)\n",
        "        generator_target=getRandomProbabilityVector(4);\n",
        "        generator_targets.append(generator_target);\n",
        "        if(seed==(len(current_start_sequence)-1)):\n",
        "          print(\"Computing reward\")\n",
        "          rewards[current_start_sequence[seed]]+=self.get_reward_value(np.array(pad_sequences([current_start_sequence], maxlen=64)), generator_target)\n",
        "          rewards_.append(self.get_reward_value(np.array(pad_sequences([current_start_sequence], maxlen=64)), generator_target))\n",
        "          continue;\n",
        "        current_rewards=[]\n",
        "        for j in range(mc_num):\n",
        "          print(\"Starting mc search\")\n",
        "          current_sequence=current_seed_sequence;\n",
        "          generator_probability_distribution=[]\n",
        "          generated_len=0;\n",
        "          while True:\n",
        "            generator_probability_distribution=self.generator.generator_model.predict_on_batch({\"labels\":np.array([generator_target]), \"sequences\":np.array(pad_sequences([current_sequence], maxlen=64))});            \n",
        "            generated_token=roll_index(generator_probability_distribution[0])\n",
        "            current_sequence=np.append(current_sequence, [generated_token])\n",
        "            generated_len+=1;\n",
        "            if(generated_token==end_token or generated_len>=max_seq_len):\n",
        "              break;\n",
        "          print(\"exited while loop\")\n",
        "          reward_value=self.get_reward_value(np.array(pad_sequences([current_sequence], maxlen=64)), generator_target)\n",
        "          current_rewards.append(reward_value)\n",
        "          final_rewards.append(reward_value)\n",
        "        print(\"Computing reward\")\n",
        "        rewards[current_start_sequence[seed]]+=sum(current_rewards)/len(current_rewards)\n",
        "        rewards_.append(sum(current_rewards)/len(current_rewards))\n",
        "      rewards=rewards/(sum(rewards)/len(rewards))   \n",
        "      print(\"Computing policy gradient\")     \n",
        "      print(np.array(states))\n",
        "      self.generator.computePolicyGradient(pad_sequences(np.array(states), maxlen=64).astype('float32'), \n",
        "                                          np.array(generator_targets).astype('float32'), \n",
        "                                          np.array(actions).astype('float32'), \n",
        "                                          np.array(rewards).astype('float32'), 3)\n",
        "\n",
        "      return final_rewards\n",
        "  def train_gen_without_using_seed_data(self, g_x, g_classifier_targets, g_y, g_epochs, state_num, mc_num, end_token, max_seq_len=100, reward_constant=1):\n",
        "    aprobs=self.generator.generator_model.predict_on_batch({\"labels\":g_classifier_targets, \"sequences\":g_x});\n",
        "    final_rewards=[]\n",
        "    for i in range(state_num):\n",
        "      print(\"Action probabilities generated\")\n",
        "      g_x_mc=g_x[np.random.shuffle(np.arange(len(g_x)))][0]\n",
        "      rewards=np.ones(len(aprobs[0]))\n",
        "      states=[]\n",
        "      actions=[]\n",
        "      generator_targets=[]\n",
        "      rewards_=[] \n",
        "      print(\"Beginning monte carlo search\")     \n",
        "      print(\"Starting state\")\n",
        "      print(g_x_mc)\n",
        "      current_start_sequence=g_x_mc[i]\n",
        "      current_start_sequence=current_start_sequence[current_start_sequence!=0]\n",
        "    for seed in range(1, len(current_start_sequence)):\n",
        "      print(\"Starting seed\")\n",
        "      current_seed_sequence=current_start_sequence[:(seed)]\n",
        "      states.append(current_seed_sequence)\n",
        "      print(len(aprobs))\n",
        "      aprob=np.zeros(len(aprobs[0]));\n",
        "      aprobi=current_start_sequence[seed]\n",
        "      aprob[aprobi]=1;\n",
        "      actions.append(aprob)\n",
        "      generator_target=getRandomProbabilityVector(4);\n",
        "      generator_targets.append(generator_target);\n",
        "      if(seed==(len(current_start_sequence)-1)):\n",
        "        print(\"Computing reward\")\n",
        "        rewards[current_start_sequence[seed]]+=self.get_reward_value(np.array(pad_sequences([current_start_sequence], maxlen=64)), generator_target)\n",
        "        rewards_.append(self.get_reward_value(np.array(pad_sequences([current_start_sequence], maxlen=64)), generator_target))\n",
        "        continue;\n",
        "      current_rewards=[]\n",
        "      for j in range(mc_num):\n",
        "        print(\"Starting mc search\")\n",
        "        current_sequence=current_seed_sequence;\n",
        "        generator_probability_distribution=[]\n",
        "        generated_len=0;\n",
        "        while True:\n",
        "          generator_probability_distribution=self.generator.generator_model.predict_on_batch({\"labels\":np.array([generator_target]), \"sequences\":np.array(pad_sequences([current_sequence], maxlen=64))});            \n",
        "          generated_token=roll_index(generator_probability_distribution[0])\n",
        "          current_sequence=np.append(current_sequence, [generated_token])\n",
        "          generated_len+=1;\n",
        "          if(generated_token==end_token or generated_len>=max_seq_len):\n",
        "            break;\n",
        "        print(\"exited while loop\")\n",
        "        reward_value=self.get_reward_value(np.array(pad_sequences([current_sequence], maxlen=64)), generator_target)\n",
        "        current_rewards.append(reward_value)\n",
        "        final_rewards.append(reward_value)\n",
        "      print(\"Computing reward\")\n",
        "      rewards[current_start_sequence[seed]]+=sum(current_rewards)/len(current_rewards)\n",
        "      rewards_.append(sum(current_rewards)/len(current_rewards))\n",
        "      rewards=rewards/(sum(rewards)/len(rewards))   \n",
        "      print(\"Computing policy gradient\")     \n",
        "      print(np.array(states))\n",
        "      self.generator.computePolicyGradient(pad_sequences(np.array(states), maxlen=64).astype('float32'), \n",
        "                                          np.array(generator_targets).astype('float32'), \n",
        "                                          np.array(actions).astype('float32'), \n",
        "                                          np.array(rewards).astype('float32'), 3)\n",
        "\n",
        "      return final_rewards\n",
        "  def get_reward_value(self,sequence,classifier_targets):\n",
        "    z=np.array([sequence]).reshape(64,)\n",
        "    print(z)\n",
        "    y=self.discriminator.compute_reward(z)\n",
        "    y2=self.classifier(sequence)[0].numpy()\n",
        "    print(y2)\n",
        "    print(classifier_targets)\n",
        "    y1=(1-self.classifier_loss(y2, classifier_targets).numpy())\n",
        "    print(\"Reward from discriminator: {}\".format(y))\n",
        "    print(\"Reward from classifier: {}\".format(y1))\n",
        "    return (((1-math.log(self.classifier_loss(self.classifier(sequence)[0], classifier_targets).numpy(), 10))*(y)**(1/2)))-0.5;\n",
        "  def get_average_score(self, sequences, classifier_targets):\n",
        "    reward_sum=0;\n",
        "    for i in range(len(sequences)):\n",
        "      x=self.get_reward_value(np.array([sequences[i]]), classifier_targets[i])\n",
        "      reward_sum+=x\n",
        "      \n",
        "     \n",
        "    return (reward_sum/len(sequences))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtEcEk3cr_op"
      },
      "source": [
        "#Pre-Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXw_frU_tg06"
      },
      "source": [
        "##Define Objects"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.losses import BinaryCrossentropy, CategoricalCrossentropy\n",
        "from keras.optimizers import adam_v2\n",
        "generator_object=Generator(generator_model, CategoricalCrossentropy(), adam_v2.Adam(lr=0.1))"
      ],
      "metadata": {
        "id": "0J0aZpos7tkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def roll_index_modified(prob_list):\n",
        "  r=random.random()\n",
        "  sum=0\n",
        "  for i in range(len(prob_list)):\n",
        "    sum+=(prob_list[i]*1.1)\n",
        "    if(sum>=r):\n",
        "      return i;\n",
        "  return 0"
      ],
      "metadata": {
        "id": "jdtmaQqRK5gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test=[0.3, 0.5, 0.15, 0.05]\n",
        "numeach=[0, 0, 0, 0]\n",
        "for i in range(10000):\n",
        "  numeach[roll_index(test)]+=1\n",
        "print(numeach)"
      ],
      "metadata": {
        "id": "RKJXo12wJEqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=generator_object.generateBatch(10)[0]\n",
        "\n",
        "for i in x:\n",
        "  #print(i)\n",
        "  sequence=i[i!=0]\n",
        "  #print(sequence)\n",
        "  sentence=\"\"\n",
        "  for j in sequence:\n",
        "    sentence=sentence+\" \"+list(gen_tokenizer.word_index.keys())[j]\n",
        "  print(sentence)"
      ],
      "metadata": {
        "id": "WoyIusUB773J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s=\"I know this is going to be one of those nights\"\n",
        "for i in range(20):\n",
        "   s+=\" \"+list(gen_tokenizer.word_index.keys())[roll_index_modified(generator_model.predict(x={\"labels\":np.array([[0, 0, 0, 1]]), \n",
        "                                  \"sequences\":pad_sequences(gen_tokenizer.texts_to_sequences([s]), \n",
        "                                                            maxlen=64)})[0])]\n",
        "print(s)"
      ],
      "metadata": {
        "id": "-oqVhWAY9o7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM7G1fwY-iSw"
      },
      "outputs": [],
      "source": [
        "from keras.backend import binary_crossentropy\n",
        "from keras.losses import BinaryCrossentropy, CategoricalCrossentropy\n",
        "from keras.optimizers import adam_v2\n",
        "discriminator_object=Discriminator(discriminator, BinaryCrossentropy(), adam_v2.Adam())\n",
        "generator_object=Generator(generator_model, CategoricalCrossentropy(), adam_v2.Adam(lr=0.1))\n",
        "gan_object=GAN(generator_object, discriminator_object, classifier, CategoricalCrossentropy())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CSz_Nextn0o"
      },
      "source": [
        "##Pretrain discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrbr-FQjBKws"
      },
      "outputs": [],
      "source": [
        "gan_object.train_disc(gan_list_states[0:100], gan_classification_onehots[0:100], gan_actions[0:100], 5, d_epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ultiAQQuNRQK"
      },
      "outputs": [],
      "source": [
        "discriminator_object.evaluate_discriminator(gan_list_states[3100:3200], gan_actions[3100:3200], generator_object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIWb47sItspp"
      },
      "source": [
        "##Generate a batch of text and evaluate the reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0Ck6DF6tbH3"
      },
      "outputs": [],
      "source": [
        "generated_values=generator_object.generateBatch(100);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a6fIsJ-vuI7"
      },
      "outputs": [],
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(classifier, to_file=\"classifer.png\", show_shapes=True, show_layer_names=True, show_layer_activations=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySR1CA2KBNaR"
      },
      "outputs": [],
      "source": [
        "#print(generator_object.generator_model.evaluate({\"labels\": classification_onehots[1000:2000],\"sequences\":token_list_states[1000:2000]}, token_actions[1000:2000]))\n",
        "\n",
        "print(gan_object.get_average_score(generated_values[0], generated_values[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71dKffR5rWtY"
      },
      "source": [
        "#Define the Award list and train the generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMNVr3zvhnqD"
      },
      "outputs": [],
      "source": [
        "reward_arr=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnrAPVfvUORQ"
      },
      "outputs": [],
      "source": [
        "print(len(gan_list_states))\n",
        "for i in range(30, 67): \n",
        "  for j in range(2):\n",
        "    reward_arr_one=gan_object.train_gen(gan_list_states[(100*i):100*(i+1)], gan_classification_onehots[(100*i):100*(i+1)], gan_actions[(100*i):100*(i+1)], 10, 5, 5, gen_tokenizer.word_index[\"endtoken\"], 20,  1)\n",
        "    gan_object.train_disc(gan_list_states[(100*i):100*(i+1)], gan_classification_onehots[(100*i):100*(i+1)], gan_actions[(100*i):100*(i+1)], 2)\n",
        "    generated_values=generator_object.generateBatch(100)\n",
        "    reward_arr.append(gan_object.get_average_score(generated_values[0], generated_values[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dG5ZpXmm9Kb"
      },
      "source": [
        "#Evaluate generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6TqklFLP9mU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IAmXfF_xC_C"
      },
      "outputs": [],
      "source": [
        "to_plot=[]\n",
        "c=0\n",
        "for i in reward_arr:\n",
        "\n",
        "  to_plot.append(i[0][0])\n",
        "  c+=1\n",
        "print(to_plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-ySvUUdbHCu"
      },
      "outputs": [],
      "source": [
        "discriminator_object.evaluate_discriminator(gan_list_states[1100:1400], gan_actions[1100:1400], generator_object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSqnqU9NasT3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel(\"Amount of Training Iterations\")\n",
        "plt.ylabel(\"Reward value\")\n",
        "plt.grid(True)\n",
        "plt.plot(to_plot)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUQCLBnOn3Jn"
      },
      "outputs": [],
      "source": [
        "print(discriminator.evaluate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKf77fKIdgw0"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkGiDsameGkT"
      },
      "outputs": [],
      "source": [
        "def total_bleu(g_object, x_data, classifier):\n",
        "  all_bleus=[]\n",
        "  for i in range(64):\n",
        "      all_bleus.append([])\n",
        "  for token_sequence in x_data:\n",
        "    print(token_sequence)\n",
        "    \n",
        "    \n",
        "    sequence_classification=classifier.predict_on_batch(np.array([token_sequence]))[0]\n",
        "    print(\"classifier\")\n",
        "    token_sequence_unpadded=token_sequence[token_sequence!=0]\n",
        "    reference_sentence=[]\n",
        "    for token in token_sequence_unpadded:\n",
        "          reference_sentence.append(list(gen_tokenizer.word_index.keys())[token])\n",
        "    reference_sentence=reference_sentence[1:]\n",
        "    for seed in range(1, len(token_sequence_unpadded)):\n",
        "        current_sequence=token_sequence_unpadded[:seed]\n",
        "        generator_probability_distribution=[]\n",
        "        generated_len=0;\n",
        "        while True:\n",
        "        # print(np.array(pad_sequences([current_sequence], maxlen=64)))\n",
        "          generator_probability_distribution=g_object.generator_model.predict_on_batch({\"labels\":np.array([sequence_classification]), \"sequences\":np.array(pad_sequences([current_sequence], maxlen=64))});\n",
        "          \n",
        "          generated_token=roll_index(generator_probability_distribution[0])\n",
        "          current_sequence=np.append(current_sequence, [generated_token])\n",
        "        # print(current_sequence)\n",
        "          \n",
        "          generated_len+=1;\n",
        "          if(generated_token==2 or generated_len>=30):\n",
        "            break;\n",
        "        sentence=[]\n",
        "        for token in current_sequence:\n",
        "          sentence.append(list(gen_tokenizer.word_index.keys())[token])\n",
        "        sentence=sentence[1:]\n",
        "        all_bleus[seed].append(sentence_bleu([sentence], reference_sentence))\n",
        "        #print(all_bleus)\n",
        "      \n",
        " \n",
        "  return all_bleus\n",
        "\n",
        "    \n",
        "         \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOwBmZ9cmgb-"
      },
      "outputs": [],
      "source": [
        "generator_object.generator_model.save_weights(\"/content/drive/MyDrive/Saved_Models/generator_weights.h5\")\n",
        "discriminator_object.discriminator_model.save_weights(\"/content/drive/MyDrive/Saved_Models/discriminator_weights.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQciZ9UjjJ_S"
      },
      "outputs": [],
      "source": [
        "all_bleus=total_bleu(generator_object, np.array(pad_sequences(token_sequences, maxlen=200))[6700:6755], classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxcHWCSRdRcn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhC-ISCarQ9M"
      },
      "outputs": [],
      "source": [
        "print(all_bleus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "jTh22E1brD5l",
        "outputId": "0803fb19-bac3-409b-f2f6-941f35d11ae0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bleu_averages=[]\n",
        "for i in all_bleus:\n",
        "  if(len(i)!=0):\n",
        "    bleu_averages.append(sum(i)/len(i))\n",
        "\n",
        "plt.plot(bleu_averages)\n",
        "plt.xlabel(\"Amount of seed words provided to the generator\")\n",
        "plt.ylabel(\"Average bleu score\")\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6CdG5Ebl3Y_"
      },
      "outputs": [],
      "source": [
        "print(sum(all_bleus)/len(all_bleus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urYq2lvfaQwf"
      },
      "outputs": [],
      "source": [
        "#print(generator_object.generator_model.evaluate({\"labels\": classification_onehots[1000:2000],\"sequences\":token_list_states[:1000]}, token_actions[1000:2000]))\n",
        "print(gan_object.get_average_score(generated_values[0], generated_values[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRUMyn_YKRal"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il9_nF4HBXgC"
      },
      "outputs": [],
      "source": [
        "!unzip tweet_emotions.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIbDlTAbylDR"
      },
      "outputs": [],
      "source": [
        "!unzip archive.zip"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ResearchProject.ipynb",
      "provenance": [],
      "mount_file_id": "1u5ixrIEMlYhW8BpZXjeNiNUkllfTaV_3",
      "authorship_tag": "ABX9TyNKhlBK4Z8cxvZ7zbkzHvpa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}